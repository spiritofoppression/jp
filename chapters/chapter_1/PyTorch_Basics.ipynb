{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17f505149f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scalar is a single number.\n",
    "* Vector is an array of numbers.\n",
    "* Matrix is a 2-D array of numbers.\n",
    "* Tensors are N-D arrays of numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create tensors by specifying the shape as arguments.  Here is a tensor with 5 rows and 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    print(\"Type: {}\".format(x.type()))\n",
    "    print(\"Shape/size: {}\".format(x.shape))\n",
    "    print(\"Values: \\n{}\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.Tensor(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 0.0461,  0.4024, -1.0115],\n",
      "        [ 0.2167, -0.6123,  0.5036]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.randn(2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's common in prototyping to create a tensor with random numbers of a specific shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.7749, 0.8208, 0.2793],\n",
      "        [0.6817, 0.2837, 0.6567]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also initialize tensors of ones or zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.zeros(2, 3))\n",
    "x = torch.ones(2, 3)\n",
    "describe(x)\n",
    "x.fill_(5)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be initialized and then filled in place. \n",
    "\n",
    "Note: operations that end in an underscore (`_`) are in place operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([3, 4])\n",
      "tensor([[5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(3,4).fill_(5)\n",
    "print(x.type())\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be initialized from a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[1., 2.],\n",
      "        [2., 4.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2,],  \n",
    "                  [2, 4,]])\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can be initialized from numpy matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.DoubleTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0.0057, 0.0316, 0.3944],\n",
      "        [0.9644, 0.1212, 0.5004]], dtype=torch.float64)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "npy = np.random.rand(2, 3)\n",
    "describe(torch.from_numpy(npy))\n",
    "print(npy.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensor Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FloatTensor has been the default tensor that we have been creating all along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(6).view(2, 3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2, 3],  \n",
    "                       [4, 5, 6]])\n",
    "describe(x)\n",
    "\n",
    "x = x.long()\n",
    "describe(x)\n",
    "\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]], dtype=torch.int64)\n",
    "describe(x)\n",
    "\n",
    "x = x.float() \n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 1.5385, -0.9757,  1.5769],\n",
      "        [ 0.3840, -0.6039, -0.5240]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 3.0771, -1.9515,  3.1539],\n",
      "        [ 0.7680, -1.2077, -1.0479]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.add(x, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[ 3.0771, -1.9515,  3.1539],\n",
      "        [ 0.7680, -1.2077, -1.0479]])\n"
     ]
    }
   ],
   "source": [
    "describe(x + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([6])\n",
      "Values: \n",
      "tensor([0, 1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = x.view(2, 3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([3])\n",
      "Values: \n",
      "tensor([3, 5, 7])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2])\n",
      "Values: \n",
      "tensor([ 3, 12])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.sum(x, dim=0))\n",
    "describe(torch.sum(x, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n"
     ]
    }
   ],
   "source": [
    "describe(torch.transpose(x, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([1, 2])\n",
      "Values: \n",
      "tensor([[0, 1]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([])\n",
      "Values: \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(6).view(2, 3)\n",
    "describe(x)\n",
    "describe(x[:1, :2])\n",
    "describe(x[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[0, 2],\n",
      "        [3, 5]])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.LongTensor([0, 2])\n",
    "describe(torch.index_select(x, dim=1, index=indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n"
     ]
    }
   ],
   "source": [
    "indices = torch.LongTensor([0, 0])\n",
    "describe(torch.index_select(x, dim=0, index=indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2])\n",
      "Values: \n",
      "tensor([0, 4])\n"
     ]
    }
   ],
   "source": [
    "row_indices = torch.arange(2).long()\n",
    "col_indices = torch.LongTensor([0, 1])\n",
    "describe(x[row_indices, col_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Long Tensors are used for indexing operations and mirror the `int64` numpy type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([3, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "torch.int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "x = torch.LongTensor([[1, 2, 3],  \n",
    "                      [4, 5, 6],\n",
    "                      [7, 8, 9]])\n",
    "describe(x)\n",
    "print(x.dtype)\n",
    "print(x.numpy().dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert a FloatTensor to a LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([3, 3])\n",
      "Values: \n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 2, 3],  \n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "x = x.long()\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Tensor initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a vector of incremental numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have an integer-based arange for indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 10).long()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "Using the tensors to do linear algebra is a foundation of modern Deep Learning practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping allows you to move the numbers in a tensor around.  One can be sure that the order is preserved.  In PyTorch, reshaping is called `view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[  0,   1,   2,   3,   4],\n",
      "          [  5,   6,   7,   8,   9],\n",
      "          [ 10,  11,  12,  13,  14],\n",
      "          [ 15,  16,  17,  18,  19]],\n",
      "\n",
      "         [[ 20,  21,  22,  23,  24],\n",
      "          [ 25,  26,  27,  28,  29],\n",
      "          [ 30,  31,  32,  33,  34],\n",
      "          [ 35,  36,  37,  38,  39]],\n",
      "\n",
      "         [[ 40,  41,  42,  43,  44],\n",
      "          [ 45,  46,  47,  48,  49],\n",
      "          [ 50,  51,  52,  53,  54],\n",
      "          [ 55,  56,  57,  58,  59]]],\n",
      "\n",
      "\n",
      "        [[[ 60,  61,  62,  63,  64],\n",
      "          [ 65,  66,  67,  68,  69],\n",
      "          [ 70,  71,  72,  73,  74],\n",
      "          [ 75,  76,  77,  78,  79]],\n",
      "\n",
      "         [[ 80,  81,  82,  83,  84],\n",
      "          [ 85,  86,  87,  88,  89],\n",
      "          [ 90,  91,  92,  93,  94],\n",
      "          [ 95,  96,  97,  98,  99]],\n",
      "\n",
      "         [[100, 101, 102, 103, 104],\n",
      "          [105, 106, 107, 108, 109],\n",
      "          [110, 111, 112, 113, 114],\n",
      "          [115, 116, 117, 118, 119]]]])\n",
      "-----\n",
      "tensor([[[[  0,   1],\n",
      "          [  2,   3],\n",
      "          [  4,   5]],\n",
      "\n",
      "         [[  6,   7],\n",
      "          [  8,   9],\n",
      "          [ 10,  11]],\n",
      "\n",
      "         [[ 12,  13],\n",
      "          [ 14,  15],\n",
      "          [ 16,  17]],\n",
      "\n",
      "         [[ 18,  19],\n",
      "          [ 20,  21],\n",
      "          [ 22,  23]]],\n",
      "\n",
      "\n",
      "        [[[ 24,  25],\n",
      "          [ 26,  27],\n",
      "          [ 28,  29]],\n",
      "\n",
      "         [[ 30,  31],\n",
      "          [ 32,  33],\n",
      "          [ 34,  35]],\n",
      "\n",
      "         [[ 36,  37],\n",
      "          [ 38,  39],\n",
      "          [ 40,  41]],\n",
      "\n",
      "         [[ 42,  43],\n",
      "          [ 44,  45],\n",
      "          [ 46,  47]]],\n",
      "\n",
      "\n",
      "        [[[ 48,  49],\n",
      "          [ 50,  51],\n",
      "          [ 52,  53]],\n",
      "\n",
      "         [[ 54,  55],\n",
      "          [ 56,  57],\n",
      "          [ 58,  59]],\n",
      "\n",
      "         [[ 60,  61],\n",
      "          [ 62,  63],\n",
      "          [ 64,  65]],\n",
      "\n",
      "         [[ 66,  67],\n",
      "          [ 68,  69],\n",
      "          [ 70,  71]]],\n",
      "\n",
      "\n",
      "        [[[ 72,  73],\n",
      "          [ 74,  75],\n",
      "          [ 76,  77]],\n",
      "\n",
      "         [[ 78,  79],\n",
      "          [ 80,  81],\n",
      "          [ 82,  83]],\n",
      "\n",
      "         [[ 84,  85],\n",
      "          [ 86,  87],\n",
      "          [ 88,  89]],\n",
      "\n",
      "         [[ 90,  91],\n",
      "          [ 92,  93],\n",
      "          [ 94,  95]]],\n",
      "\n",
      "\n",
      "        [[[ 96,  97],\n",
      "          [ 98,  99],\n",
      "          [100, 101]],\n",
      "\n",
      "         [[102, 103],\n",
      "          [104, 105],\n",
      "          [106, 107]],\n",
      "\n",
      "         [[108, 109],\n",
      "          [110, 111],\n",
      "          [112, 113]],\n",
      "\n",
      "         [[114, 115],\n",
      "          [116, 117],\n",
      "          [118, 119]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 120)\n",
    "\n",
    "print(x.view(2, 3, 4, 5))\n",
    "print(\"-----\")\n",
    "print(x.view(5, 4, 3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use view to add size-1 dimensions, which can be useful for combining with other tensors.  This is called broadcasting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[0, 1, 2, 3]])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[ 0,  2,  4,  6],\n",
      "        [ 4,  6,  8, 10],\n",
      "        [ 8, 10, 12, 14]])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [10, 11, 12, 13]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view(3, 4)\n",
    "y = torch.arange(4).view(1, 4)\n",
    "z = torch.arange(3).view(3, 1)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)\n",
    "print(x + y)\n",
    "print(x + z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsqueeze and squeeze will add and remove 1-dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).view(3, 4)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.unsqueeze(dim=1)\n",
    "print(x.shape)\n",
    "\n",
    "x = x.squeeze()\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all of the standard mathematics operations apply (such as `add` below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[0.6662, 0.3343, 0.7893, 0.3216],\n",
      "        [0.5247, 0.6688, 0.8436, 0.4265],\n",
      "        [0.9561, 0.0770, 0.4108, 0.0014]])\n",
      "--\n",
      "torch.add(x, x): \n",
      " tensor([[1.3324, 0.6686, 1.5786, 0.6433],\n",
      "        [1.0494, 1.3377, 1.6872, 0.8530],\n",
      "        [1.9123, 0.1540, 0.8216, 0.0028]])\n",
      "--\n",
      "x+x: \n",
      " tensor([[1.3324, 0.6686, 1.5786, 0.6433],\n",
      "        [1.0494, 1.3377, 1.6872, 0.8530],\n",
      "        [1.9123, 0.1540, 0.8216, 0.0028]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,4)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"--\")\n",
    "print(\"torch.add(x, x): \\n\", torch.add(x, x))\n",
    "print(\"--\")\n",
    "print(\"x+x: \\n\", x + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convention of `_` indicating in-place operations continues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor([[ 0,  2,  4,  6],\n",
      "        [ 8, 10, 12, 14],\n",
      "        [16, 18, 20, 22]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(x)\n",
    "print(x.add_(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many operations for which reduce a dimension.  Such as sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "---\n",
      "Summing across rows (dim=0): \n",
      " tensor([12, 15, 18, 21])\n",
      "---\n",
      "Summing across columns (dim=1): \n",
      " tensor([ 6, 22, 38])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"---\")\n",
    "print(\"Summing across rows (dim=0): \\n\", x.sum(dim=0))\n",
    "print(\"---\")\n",
    "print(\"Summing across columns (dim=1): \\n\", x.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing, Slicing, Joining and Mutating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "---\n",
      "x[:2, :2]: \n",
      " tensor([[0, 1],\n",
      "        [3, 4]])\n",
      "---\n",
      "x[0][1]: \n",
      " tensor(1)\n",
      "---\n",
      "Setting [0][1] to be 8\n",
      "tensor([[0, 8, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"---\")\n",
    "print(\"x[:2, :2]: \\n\", x[:2, :2])\n",
    "print(\"---\")\n",
    "print(\"x[0][1]: \\n\", x[0][1])\n",
    "print(\"---\")\n",
    "print(\"Setting [0][1] to be 8\")\n",
    "x[0][1] = 8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can select a subset of a tensor using the `index_select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "tensor([[0, 1, 2],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "tensor([[0, 2],\n",
      "        [3, 5],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9).view(3,3)\n",
    "print(x)\n",
    "\n",
    "print(\"---\")\n",
    "indices = torch.LongTensor([0, 2])\n",
    "print(torch.index_select(x, dim=0, index=indices))\n",
    "\n",
    "print(\"---\")\n",
    "indices = torch.LongTensor([0, 2])\n",
    "print(torch.index_select(x, dim=1, index=indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use numpy-style advanced indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "tensor([[0, 1, 2],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "tensor([[0, 2],\n",
      "        [3, 5],\n",
      "        [6, 8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9).view(3,3)\n",
    "indices = torch.LongTensor([0, 2])\n",
    "\n",
    "print(x[indices])\n",
    "print(\"---\")\n",
    "print(x[indices, :])\n",
    "print(\"---\")\n",
    "print(x[:, indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine tensors by concatenating them.  First, concatenating on the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([4, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 6])\n",
      "Values: \n",
      "tensor([[0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 2, 3])\n",
      "Values: \n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).view(2,3)\n",
    "describe(x)\n",
    "describe(torch.cat([x, x], dim=0))\n",
    "describe(torch.cat([x, x], dim=1))\n",
    "describe(torch.stack([x, x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can concentate along the first dimension.. the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "torch.Size([3, 9])\n",
      "tensor([[0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
      "        [3, 4, 5, 3, 4, 5, 3, 4, 5],\n",
      "        [6, 7, 8, 6, 7, 8, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9).view(3,3)\n",
    "\n",
    "print(x)\n",
    "print(\"---\")\n",
    "new_x = torch.cat([x, x, x], dim=1)\n",
    "print(new_x.shape)\n",
    "print(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also concatenate on a new 0th dimension to \"stack\" the tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "---\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([[[0, 1, 2],\n",
      "         [3, 4, 5],\n",
      "         [6, 7, 8]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5],\n",
      "         [6, 7, 8]],\n",
      "\n",
      "        [[0, 1, 2],\n",
      "         [3, 4, 5],\n",
      "         [6, 7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(9).view(3,3)\n",
    "print(x)\n",
    "print(\"---\")\n",
    "new_x = torch.stack([x, x, x])\n",
    "print(new_x.shape)\n",
    "print(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra Tensor Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing allows you to switch the dimensions to be on different axis. So we can make it so all the rows are columsn and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "---\n",
      "x.tranpose(1, 0): \n",
      " tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 12).view(3,4)\n",
    "print(\"x: \\n\", x) \n",
    "print(\"---\")\n",
    "print(\"x.tranpose(1, 0): \\n\", x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A three dimensional tensor would represent a batch of sequences, where each sequence item has a feature vector.  It is common to switch the batch and sequence dimensions so that we can more easily index the sequence in a sequence model. \n",
    "\n",
    "Note: Transpose will only let you swap 2 axes.  Permute (in the next cell) allows for multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: \n",
      " torch.Size([3, 4, 5])\n",
      "x: \n",
      " tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n",
      "-----\n",
      "x.transpose(1, 0).shape: \n",
      " torch.Size([4, 3, 5])\n",
      "x.transpose(1, 0): \n",
      " tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [20, 21, 22, 23, 24],\n",
      "         [40, 41, 42, 43, 44]],\n",
      "\n",
      "        [[ 5,  6,  7,  8,  9],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [45, 46, 47, 48, 49]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [50, 51, 52, 53, 54]],\n",
      "\n",
      "        [[15, 16, 17, 18, 19],\n",
      "         [35, 36, 37, 38, 39],\n",
      "         [55, 56, 57, 58, 59]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_size = 4\n",
    "feature_size = 5\n",
    "\n",
    "x = torch.arange(batch_size * seq_size * feature_size).view(batch_size, seq_size, feature_size)\n",
    "\n",
    "print(\"x.shape: \\n\", x.shape)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"-----\")\n",
    "\n",
    "print(\"x.transpose(1, 0).shape: \\n\", x.transpose(1, 0).shape)\n",
    "print(\"x.transpose(1, 0): \\n\", x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permute is a more general version of tranpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: \n",
      " torch.Size([3, 4, 5])\n",
      "x: \n",
      " tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n",
      "-----\n",
      "x.permute(1, 0, 2).shape: \n",
      " torch.Size([4, 3, 5])\n",
      "x.permute(1, 0, 2): \n",
      " tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [20, 21, 22, 23, 24],\n",
      "         [40, 41, 42, 43, 44]],\n",
      "\n",
      "        [[ 5,  6,  7,  8,  9],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [45, 46, 47, 48, 49]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [50, 51, 52, 53, 54]],\n",
      "\n",
      "        [[15, 16, 17, 18, 19],\n",
      "         [35, 36, 37, 38, 39],\n",
      "         [55, 56, 57, 58, 59]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_size = 4\n",
    "feature_size = 5\n",
    "\n",
    "x = torch.arange(batch_size * seq_size * feature_size).view(batch_size, seq_size, feature_size)\n",
    "\n",
    "print(\"x.shape: \\n\", x.shape)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"-----\")\n",
    "\n",
    "print(\"x.permute(1, 0, 2).shape: \\n\", x.permute(1, 0, 2).shape)\n",
    "print(\"x.permute(1, 0, 2): \\n\", x.permute(1, 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is `mm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4790,  0.8539, -0.2285],\n",
       "        [ 0.3081,  1.1171,  0.1585]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 2])\n",
      "Values: \n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[ 3.,  6.],\n",
      "        [12., 24.]])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.arange(6).view(2, 3).float()\n",
    "describe(x1)\n",
    "\n",
    "x2 = torch.ones(3, 2)\n",
    "x2[:, 1] += 1\n",
    "describe(x2)\n",
    "\n",
    "describe(torch.mm(x1, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.],\n",
      "        [1., 2.]])\n",
      "tensor([[ 6., 12.],\n",
      "        [22., 44.],\n",
      "        [38., 76.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0, 12).view(3,4).float()\n",
    "print(x)\n",
    "\n",
    "x2 = torch.ones(4, 2)\n",
    "x2[:, 1] += 1\n",
    "print(x2)\n",
    "\n",
    "print(x.mm(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the [PyTorch Math Operations Documentation](https://pytorch.org/docs/stable/torch.html#math-operations) for more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 9.]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2.0, 3.0]], requires_grad=True)\n",
    "z = 3 * x\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this small snippet, you can see the gradient computations at work.  We create a tensor and multiply it by 3.  Then, we create a scalar output using `sum()`.  A Scalar output is needed as the the loss variable. Then, called backward on the loss means it computes its rate of change with respect to the inputs.  Since the scalar was created with sum, each position in z and x are independent with respect to the loss scalar. \n",
    "\n",
    "The rate of change of x with respect to the output is just the constant 3 that we multiplied x by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      " tensor([[2., 3.]], requires_grad=True)\n",
      "---\n",
      "z = 3*x: \n",
      " tensor([[6., 9.]], grad_fn=<MulBackward0>)\n",
      "---\n",
      "loss = z.sum(): \n",
      " tensor(15., grad_fn=<SumBackward0>)\n",
      "---\n",
      "after loss.backward(), x.grad: \n",
      " tensor([[3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[2.0, 3.0]], requires_grad=True)\n",
    "print(\"x: \\n\", x)\n",
    "print(\"---\")\n",
    "z = 3 * x\n",
    "print(\"z = 3*x: \\n\", z)\n",
    "print(\"---\")\n",
    "\n",
    "loss = z.sum()\n",
    "print(\"loss = z.sum(): \\n\", loss)\n",
    "print(\"---\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"after loss.backward(), x.grad: \\n\", x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Computing a conditional gradient\n",
    "\n",
    "$$ \\text{ Find the gradient of f(x) at x=1 } $$\n",
    "$$ {} $$\n",
    "$$ f(x)=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "    sin(x) \\text{ if } x>0 \\\\\n",
    "    cos(x) \\text{ otherwise } \\\\\n",
    "\\end{array}\n",
    "\\right.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if (x.data > 0).all():\n",
    "        return torch.sin(x)\n",
    "    else:\n",
    "        return torch.cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5403])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could apply this to a larger vector too, but we need to make sure the output is a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-a63e7f90faf5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# this is meant to break!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ms:\\Anaconda_3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ms:\\Anaconda_3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ms:\\Anaconda_3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 0.5], requires_grad=True)\n",
    "y = f(x)\n",
    "# this is meant to break!\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the output a scalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5403, 0.8776])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 0.5], requires_grad=True)\n",
    "y = f(x)\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but there was an issue.. this isn't right for this edge case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8415,  0.8415])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, -1], requires_grad=True)\n",
    "y = f(x)\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4794, 0.8415])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([-0.5, -1], requires_grad=True)\n",
    "y = f(x)\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because we aren't doing the boolean computation and subsequent application of cos and sin on an elementwise basis.  So, to solve this, it is common to use masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5403, 0.8415])\n"
     ]
    }
   ],
   "source": [
    "def f2(x):\n",
    "    mask = torch.gt(x, 0).float()\n",
    "    return mask * torch.sin(x) + (1 - mask) * torch.cos(x)\n",
    "\n",
    "x = torch.tensor([1.0, -1], requires_grad=True)\n",
    "y = f2(x)\n",
    "y.sum().backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_grad(x):\n",
    "    if x.grad is None:\n",
    "        print(\"No gradient information\")\n",
    "    else:\n",
    "        print(\"Gradient: \\n{}\".format(x.grad))\n",
    "        print(\"Gradient Function: {}\".format(x.grad_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "No gradient information\n",
      "--------\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([2, 2])\n",
      "Values: \n",
      "tensor([[21., 21.],\n",
      "        [21., 21.]], grad_fn=<AddBackward0>)\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([])\n",
      "Values: \n",
      "21.0\n",
      "No gradient information\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\Anaconda_3\\lib\\site-packages\\torch\\autograd\\__init__.py:156: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  ..\\torch\\csrc\\autograd\\engine.cpp:976.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: \n",
      "tensor([[2.2500, 2.2500],\n",
      "        [2.2500, 2.2500]], grad_fn=<CopyBackwards>)\n",
      "Gradient Function: None\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "describe(x)\n",
    "describe_grad(x)\n",
    "print(\"--------\")\n",
    "\n",
    "y = (x + 2) * (x + 5) + 3\n",
    "describe(y)\n",
    "z = y.mean()\n",
    "describe(z)\n",
    "describe_grad(x)\n",
    "print(\"--------\")\n",
    "z.backward(create_graph=True, retain_graph=True)\n",
    "describe_grad(x)\n",
    "print(\"--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x17f5396fd88>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's operations can seamlessly be used on the GPU or on the CPU.  There are a couple basic operations for interacting in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 3])\n",
      "Values: \n",
      "tensor([[0.9149, 0.3993, 0.1100],\n",
      "        [0.2541, 0.4333, 0.4451],\n",
      "        [0.4966, 0.7865, 0.6604]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,3)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(3, 3).to(device)\n",
    "describe(x)\n",
    "print(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will break!\n",
    "y = torch.rand(3, 3)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.to(cpu_device)\n",
    "x = x.to(cpu_device)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # only is GPU is available\n",
    "    a = torch.rand(3,3).to(device='cuda:0') #  CUDA Tensor\n",
    "    print(a)\n",
    "    \n",
    "    b = torch.rand(3,3).cuda()\n",
    "    print(b)\n",
    "\n",
    "    print(a + b)\n",
    "\n",
    "    a = a.cpu() # Error expected\n",
    "    print(a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercises\n",
    "\n",
    "Some of these exercises require operations not covered in the notebook.  You will have to look at [the documentation](https://pytorch.org/docs/) (on purpose!)\n",
    "\n",
    "\n",
    "(Answers are at the bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Create a 2D tensor and then add a dimension of size 1 inserted at the 0th axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 1, 3])\n",
      "Values: \n",
      "tensor([[[0, 1, 2]],\n",
      "\n",
      "        [[3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6).view(2, 3)\n",
    "describe(x)\n",
    "x = x.unsqueeze(1)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Remove the extra dimension you just added to the previous tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.LongTensor\n",
      "Shape/size: torch.Size([2, 3])\n",
      "Values: \n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "x = x.squeeze(1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Create a random tensor of shape 5x3 in the interval [3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([5, 3])\n",
      "Values: \n",
      "tensor([[4.9291, 6.9075, 4.4750],\n",
      "        [5.5805, 6.8795, 7.0380],\n",
      "        [6.1244, 6.5798, 6.8897],\n",
      "        [5.7664, 5.4535, 7.5273],\n",
      "        [7.9496, 6.9264, 5.1257]])\n"
     ]
    }
   ],
   "source": [
    "x = 4+torch.rand(5, 3)*(8-4)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Create a tensor with values from a normal distribution (mean=0, std=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([5, 1])\n",
      "Values: \n",
      "tensor([[ 1.3299],\n",
      "        [-0.0671],\n",
      "        [ 0.5094],\n",
      "        [-0.3453],\n",
      "        [-0.5296]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,1)\n",
    "describe(x)\n",
    "x[2, 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Retrieve the indexes of all the non zero elements in the tensor torch.Tensor([1, 1, 1, 0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([5, 1])\n",
      "Values: \n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 1)\n",
    "for i in range(5):\n",
    "    if x[i, 0] == 0:\n",
    "        y[i, 0] = 0\n",
    "    else:\n",
    "        y[i, 0] = 1\n",
    "describe(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "Create a random tensor of size (3,1) and then horizonally stack 4 copies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 1])\n",
      "Values: \n",
      "tensor([[0.5896],\n",
      "        [0.7928],\n",
      "        [0.0367]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4])\n",
      "Values: \n",
      "tensor([[0.5896, 0.5896, 0.5896, 0.5896],\n",
      "        [0.7928, 0.7928, 0.7928, 0.7928],\n",
      "        [0.0367, 0.0367, 0.0367, 0.0367]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 1)\n",
    "describe(x)\n",
    "x = torch.cat([x, x, x, x], 1)\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Return the batch matrix-matrix product of two 3 dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4, 5])\n",
      "Values: \n",
      "tensor([[[4.4899e-02, 2.0082e-01, 5.6099e-01, 4.0732e-01, 6.0775e-01],\n",
      "         [3.8110e-01, 1.7268e-01, 5.6638e-01, 8.3864e-01, 1.9978e-01],\n",
      "         [9.3829e-01, 6.2306e-01, 9.9302e-01, 4.1706e-02, 2.1394e-02],\n",
      "         [6.6963e-01, 3.3555e-02, 9.0252e-02, 7.5424e-01, 7.4271e-01]],\n",
      "\n",
      "        [[4.4390e-02, 8.5979e-01, 4.0278e-01, 8.7121e-01, 2.0658e-01],\n",
      "         [2.4565e-02, 1.6854e-01, 4.2249e-01, 6.3777e-04, 4.6593e-01],\n",
      "         [2.5046e-01, 3.8362e-01, 4.9993e-01, 5.2806e-01, 1.0909e-02],\n",
      "         [7.5719e-01, 1.9355e-01, 8.0471e-01, 9.5816e-01, 3.3325e-01]],\n",
      "\n",
      "        [[5.5612e-01, 7.7569e-01, 4.6039e-01, 8.1637e-02, 7.2460e-01],\n",
      "         [7.3787e-01, 2.2607e-01, 1.6334e-01, 4.1274e-01, 5.9803e-01],\n",
      "         [2.4714e-01, 4.8798e-01, 2.8393e-01, 3.0521e-01, 4.3157e-01],\n",
      "         [3.0943e-01, 8.8407e-01, 6.3138e-01, 9.4747e-01, 9.5146e-01]]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 5, 4])\n",
      "Values: \n",
      "tensor([[[0.5817, 0.2920, 0.8040, 0.4826],\n",
      "         [0.8701, 0.3932, 0.9422, 0.5660],\n",
      "         [0.3889, 0.1738, 0.8700, 0.5650],\n",
      "         [0.6730, 0.7021, 0.4742, 0.2486],\n",
      "         [0.3508, 0.4777, 0.8105, 0.4635]],\n",
      "\n",
      "        [[0.3732, 0.4320, 0.5821, 0.8991],\n",
      "         [0.8773, 0.4708, 0.5663, 0.9534],\n",
      "         [0.7197, 0.4472, 0.7964, 0.7277],\n",
      "         [0.3657, 0.7023, 0.5396, 0.3144],\n",
      "         [0.4608, 0.9706, 0.6502, 0.0872]],\n",
      "\n",
      "        [[0.8186, 0.6074, 0.7460, 0.5021],\n",
      "         [0.6989, 0.6495, 0.9390, 0.7639],\n",
      "         [0.2812, 0.7620, 0.4703, 0.8306],\n",
      "         [0.3246, 0.3532, 0.9624, 0.5881],\n",
      "         [0.5701, 0.5513, 0.5326, 0.5210]]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4, 4])\n",
      "Values: \n",
      "tensor([[[0.9063, 0.7659, 1.3991, 0.8353],\n",
      "         [1.2267, 0.9619, 1.5215, 0.9028],\n",
      "         [1.5097, 0.7311, 2.2425, 1.3869],\n",
      "         [1.2219, 1.1088, 1.6082, 0.9249]],\n",
      "\n",
      "        [[1.4745, 1.4164, 1.4379, 1.4447],\n",
      "         [0.6760, 0.7316, 0.7495, 0.5311],\n",
      "         [0.9880, 0.8938, 1.0532, 1.1217],\n",
      "         [1.5355, 1.7745, 1.9250, 1.7812]],\n",
      "\n",
      "        [[1.5664, 1.6207, 1.8242, 1.6797],\n",
      "         [1.2829, 1.1950, 1.5553, 1.2332],\n",
      "         [0.9683, 1.0291, 1.2997, 1.1370],\n",
      "         [1.8986, 2.1025, 2.7765, 2.4081]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,4,5)\n",
    "b=torch.rand(3,5,4)\n",
    "describe(a)\n",
    "describe(b)\n",
    "describe(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4, 5])\n",
      "Values: \n",
      "tensor([[[0.0655, 0.9826, 0.9773, 0.1558, 0.6236],\n",
      "         [0.9133, 0.5968, 0.7151, 0.8724, 0.7122],\n",
      "         [0.4096, 0.0844, 0.2642, 0.2023, 0.5101],\n",
      "         [0.1460, 0.8175, 0.5985, 0.2834, 0.2103]],\n",
      "\n",
      "        [[0.6638, 0.3803, 0.5955, 0.5470, 0.5622],\n",
      "         [0.4308, 0.4020, 0.3828, 0.6928, 0.1777],\n",
      "         [0.5117, 0.2968, 0.0248, 0.6407, 0.6541],\n",
      "         [0.9449, 0.8285, 0.9894, 0.1071, 0.0648]],\n",
      "\n",
      "        [[0.6555, 0.7450, 0.7203, 0.1707, 0.8913],\n",
      "         [0.0654, 0.6332, 0.3086, 0.3201, 0.8951],\n",
      "         [0.9325, 0.6941, 0.7241, 0.5195, 0.1934],\n",
      "         [0.6304, 0.9300, 0.7095, 0.0515, 0.3947]]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([5, 4])\n",
      "Values: \n",
      "tensor([[0.6595, 0.4078, 0.3445, 0.0616],\n",
      "        [0.5512, 0.6339, 0.0456, 0.5955],\n",
      "        [0.6505, 0.6866, 0.8257, 0.2046],\n",
      "        [0.6629, 0.5462, 0.3548, 0.4695],\n",
      "        [0.9934, 0.5161, 0.3650, 0.5073]])\n",
      "Type: torch.FloatTensor\n",
      "Shape/size: torch.Size([3, 4, 4])\n",
      "Values: \n",
      "tensor([[[1.9433, 1.7276, 1.1573, 1.1786],\n",
      "         [2.6822, 2.0859, 1.5017, 1.3289],\n",
      "         [1.1293, 0.7757, 0.6211, 0.4833],\n",
      "         [1.3329, 1.2520, 0.7590, 0.8580]],\n",
      "\n",
      "        [[1.9558, 1.5096, 1.1370, 0.9312],\n",
      "         [1.3904, 1.1634, 0.7934, 0.7597],\n",
      "         [1.5917, 1.1014, 0.6764, 0.8460],\n",
      "         [1.8587, 1.6818, 1.2418, 0.8371]],\n",
      "\n",
      "        [[2.3100, 1.7874, 1.2404, 1.1637],\n",
      "         [1.6942, 1.2767, 0.7464, 1.0486],\n",
      "         [2.0051, 1.7011, 1.2057, 0.9610],\n",
      "         [1.8161, 1.5656, 1.0077, 0.9622]]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(3,4,5)\n",
    "b=torch.rand(5,4)\n",
    "describe(a)\n",
    "describe(b)\n",
    "describe(torch.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers still below.. Keep Going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Create a 2D tensor and then add a dimension of size 1 inserted at the 0th axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)\n",
    "a = a.unsqueeze(0)\n",
    "print(a)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 2 \n",
    "\n",
    "Remove the extra dimension you just added to the previous tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.squeeze(0)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Create a random tensor of shape 5x3 in the interval [3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 + torch.rand(5, 3) * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Create a tensor with values from a normal distribution (mean=0, std=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3,3)\n",
    "a.normal_(mean=0, std=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Retrieve the indexes of all the non zero elements in the tensor torch.Tensor([1, 1, 1, 0, 1])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [4]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1, 1, 1, 0, 1])\n",
    "torch.nonzero(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "\n",
    "Create a random tensor of size (3,1) and then horizonally stack 4 copies together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2586, 0.2586, 0.2586, 0.2586],\n",
       "        [0.2099, 0.2099, 0.2099, 0.2099],\n",
       "        [0.3873, 0.3873, 0.3873, 0.3873]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,1)\n",
    "a.expand(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7\n",
    "\n",
    "Return the batch matrix-matrix product of two 3 dimensional matrices (a=torch.rand(3,4,5), b=torch.rand(3,5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7930, 1.7822, 1.9577, 1.3544],\n",
       "         [0.5916, 1.3901, 1.4047, 1.3880],\n",
       "         [0.5592, 0.9601, 0.9455, 1.3010],\n",
       "         [0.7200, 1.5366, 1.9125, 1.0774]],\n",
       "\n",
       "        [[1.3656, 1.6819, 1.1158, 1.4991],\n",
       "         [0.7533, 0.6934, 0.8819, 0.7267],\n",
       "         [0.8952, 1.3834, 1.3718, 1.3493],\n",
       "         [1.2043, 1.3467, 0.7761, 1.3627]],\n",
       "\n",
       "        [[1.6966, 1.7142, 0.8968, 1.1402],\n",
       "         [1.3548, 1.4573, 0.7535, 0.8257],\n",
       "         [1.0757, 1.2090, 0.7561, 0.6798],\n",
       "         [1.8444, 2.3710, 1.0746, 1.3107]]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,4,5)\n",
    "b = torch.rand(3,5,4)\n",
    "torch.bmm(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Exercise 8\n",
    "\n",
    "Return the batch matrix-matrix product of a 3D matrix and a 2D matrix (a=torch.rand(3,4,5), b=torch.rand(5,4))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.1570, 0.7510, 1.4535, 1.1073],\n",
       "         [0.8361, 0.3259, 0.9052, 1.0406],\n",
       "         [0.5271, 0.6889, 1.2667, 0.8973],\n",
       "         [1.0088, 0.8406, 1.4871, 1.0364]],\n",
       "\n",
       "        [[0.4588, 0.7370, 1.2826, 1.0181],\n",
       "         [0.8666, 0.8858, 1.9161, 0.9384],\n",
       "         [0.8010, 0.6937, 1.3537, 0.7119],\n",
       "         [0.8059, 0.4964, 0.9559, 1.1389]],\n",
       "\n",
       "        [[0.7673, 0.7080, 1.3059, 0.7986],\n",
       "         [1.2132, 1.0041, 2.0155, 1.2557],\n",
       "         [0.8435, 0.6616, 1.2377, 0.7562],\n",
       "         [1.0520, 0.7462, 1.1520, 1.4115]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(3,4,5)\n",
    "b = torch.rand(5,4)\n",
    "torch.bmm(a, b.unsqueeze(0).expand(a.size(0), *b.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
